{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Fruit-veg recognition.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ade9e0d5"
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from cv2 import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "#from keras.layers.normalization import BatchNormalization\n",
        "from tensorflow.keras.layers import (\n",
        "    BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
        ")\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "id": "ade9e0d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmFLI1jaBiZT",
        "outputId": "96d1c211-4403-45a1-cd4a-05f122c8e230"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "QmFLI1jaBiZT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "0d25e817",
        "outputId": "f96c4f7e-7133-41f5-deec-98aed9ccd322"
      },
      "source": [
        "val_path=\"../input/fruit-and-vegetable-image-recognition/validation\"\n",
        "train_path=\"../input/fruit-and-vegetable-image-recognition/train\"\n",
        "\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(train_path,\n",
        "                                                               seed=2509,\n",
        "                                                               image_size=(224, 224),\n",
        "                                                              batch_size=32)\n",
        "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(val_path,\n",
        "                                                              seed=2509,\n",
        "                                                              image_size=(224, 224),\n",
        "                                                              shuffle=False,\n",
        "                                                              batch_size=32)"
      ],
      "id": "0d25e817",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2e401fe253b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2509\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                                \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                                               batch_size=32)\n\u001b[0m\u001b[1;32m      8\u001b[0m val_dataset = tf.keras.preprocessing.image_dataset_from_directory(val_path,\n\u001b[1;32m      9\u001b[0m                                                               \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2509\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m       \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m       follow_links=follow_links)\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlabel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/preprocessing/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0msubdirs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/fruit-and-vegetable-image-recognition/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f6856c0"
      },
      "source": [
        "class_names = train_dataset.class_names\n",
        "print(len(class_names))"
      ],
      "id": "8f6856c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51861b76"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size = (3, 3), activation='relu', input_shape=(224,224,3)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(96, kernel_size=(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "#model.add(Dropout(0.3))\n",
        "model.add(Dense(len(class_names),activation='softmax'))"
      ],
      "id": "51861b76",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39196871"
      },
      "source": [
        "model.compile(\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer = tf.keras.optimizers.Adam(lr=0.001),\n",
        "    metrics = [\"accuracy\"])\n",
        "model.summary()"
      ],
      "id": "39196871",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e8b0ece"
      },
      "source": [
        "history = model.fit(x=train_dataset,\n",
        "                    epochs= 20,\n",
        "                    validation_data=val_dataset)"
      ],
      "id": "4e8b0ece",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68f4fb07"
      },
      "source": [
        "plt.plot(history.history['val_loss'])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title(\"Model Loss\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel('Time')\n",
        "plt.legend(['val_loss', 'loss'], loc='upper left')\n",
        "plt.show()"
      ],
      "id": "68f4fb07",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac9195da"
      },
      "source": [
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title(\"Model Accuracy\")\n",
        "plt.ylabel(\"Epochs\")\n",
        "plt.xlabel('Time')\n",
        "plt.legend(['val_accuracy', 'acc'], loc='upper left')\n",
        "plt.show()"
      ],
      "id": "ac9195da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bb3efa6"
      },
      "source": [
        "from keras_preprocessing import image\n",
        "image_path=\"../input/fruit-and-vegetable-image-recognition/test/corn/Image_10.jpg\"\n",
        "img = image.load_img(image_path, target_size=(224,224,3))\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "images = np.vstack([x])\n",
        "pred = model.predict(images, batch_size=32)\n",
        "label = np.argmax(pred, axis=1)\n",
        "print(\"Actual: \"+image_path.split(\"/\")[-2])\n",
        "print(\"Predicted: \"+class_names[np.argmax(pred)])"
      ],
      "id": "3bb3efa6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b431b7b"
      },
      "source": [
        "from keras_preprocessing import image\n",
        "image_path=\"../input/fruit-and-vegetable-image-recognition/test/cabbage/Image_7.jpg\"\n",
        "img = image.load_img(image_path, target_size=(224,224,3))\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "images = np.vstack([x])\n",
        "pred = model.predict(images, batch_size=32)\n",
        "label = np.argmax(pred, axis=1)\n",
        "print(\"Actual: \"+image_path.split(\"/\")[-2])\n",
        "print(\"Predicted: \"+class_names[np.argmax(pred)])\n"
      ],
      "id": "1b431b7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f0d47f3"
      },
      "source": [
        "from keras_preprocessing import image\n",
        "image_path=\"../input/fruit-and-vegetable-image-recognition/test/banana/Image_5.jpg\"\n",
        "img = image.load_img(image_path, target_size=(224,224,3))\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "images = np.vstack([x])\n",
        "pred = model.predict(images, batch_size=32)\n",
        "label = np.argmax(pred, axis=1)\n",
        "print(\"Actual: \"+image_path.split(\"/\")[-2])\n",
        "print(\"Predicted: \"+class_names[np.argmax(pred)])"
      ],
      "id": "5f0d47f3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0c5cc11"
      },
      "source": [
        "from keras_preprocessing import image\n",
        "image_path=\"../input/fruit-and-vegetable-image-recognition/test/tomato/Image_2.jpg\"\n",
        "img = image.load_img(image_path, target_size=(224,224,3))\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "images = np.vstack([x])\n",
        "pred = model.predict(images, batch_size=32)\n",
        "print(\"Actual: \"+image_path.split(\"/\")[-2])\n",
        "print(\"Predicted: \"+class_names[np.argmax(pred)])"
      ],
      "id": "f0c5cc11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f7b3083"
      },
      "source": [
        ""
      ],
      "id": "0f7b3083",
      "execution_count": null,
      "outputs": []
    }
  ]
}